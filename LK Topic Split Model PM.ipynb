{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "### Read in Data / Data Prep", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 1, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "sc = spark.sparkContext"
        }, 
        {
            "execution_count": 2, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by DSX for sharing."
        }, 
        {
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "dash_users = dash_users.drop(\"TENANT_ID\")"
        }, 
        {
            "execution_count": 4, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "interestmap_config = spark.read.jdbc(properties_6e54ed0ab7e1422aa339fd754b921514['jdbcurl'], table='CAO.INTERESTMAP_CONFIG', properties=properties_6e54ed0ab7e1422aa339fd754b921514)\n#interestmap_config.head()"
        }, 
        {
            "execution_count": 5, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "item_tags = spark.read.jdbc(properties_6e54ed0ab7e1422aa339fd754b921514['jdbcurl'], table='CAO.TAG2', properties=properties_6e54ed0ab7e1422aa339fd754b921514)\n#item_tags.head()"
        }, 
        {
            "execution_count": 6, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "source": "item_topic = interestmap_config.join(item_tags, (interestmap_config.TAGID == item_tags.TAGID) & (interestmap_config.GROUPID == item_tags.GROUPID),\"right_outer\")\nitem_topic = item_topic.select(\"INTEREST_ID\", \"INTEREST_LABEL\",\"TARGETID\").distinct()\n#item_topic.head()"
        }, 
        {
            "execution_count": 7, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "item_topic = item_topic.withColumnRenamed('TARGETID', 'ITEM_ID')\n#item_topic.head()"
        }, 
        {
            "execution_count": 8, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "items1 = spark.read.jdbc(properties_6e54ed0ab7e1422aa339fd754b921514['jdbcurl'], table='CAO.USERS_ITEMHISTORY_FULL', properties=properties_6e54ed0ab7e1422aa339fd754b921514)\nitems1 = items1.select(\"USER_ID\", \"ITEM_ID\", \"ITEM_COMPLETIONDATE\").distinct()\n#items.head()"
        }, 
        {
            "execution_count": 9, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "itemlist = spark.read.jdbc(properties_6e54ed0ab7e1422aa339fd754b921514['jdbcurl'], table='CAO.WRK_RE2_ITEMLIST2', properties=properties_6e54ed0ab7e1422aa339fd754b921514)\nitemlist = itemlist.select(\"ITEMID\").distinct()\n#items.head()"
        }, 
        {
            "execution_count": 10, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "itemlist = itemlist.withColumnRenamed('ITEMID', 'ITEM_ID')"
        }, 
        {
            "execution_count": 11, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "items = items1.join(itemlist, \"ITEM_ID\", \"inner\")\n#dash_valid_items.head()"
        }, 
        {
            "execution_count": 12, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "dash_valid_items = items.join(item_topic, \"ITEM_ID\", \"inner\")\n#dash_valid_items.head()"
        }, 
        {
            "execution_count": 13, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": false
            }, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "root\n |-- USER_ID: string (nullable = false)\n |-- BAND: integer (nullable = true)\n |-- TENURE: integer (nullable = true)\n |-- BUSINESSUNITGROUP: integer (nullable = true)\n |-- BUSINESSUNIT: integer (nullable = true)\n |-- CITY: integer (nullable = true)\n |-- STATE: integer (nullable = true)\n |-- COUNTRYCODE: integer (nullable = true)\n |-- COUNTRY: integer (nullable = true)\n |-- REGION: integer (nullable = true)\n |-- GEOGRAPHY: integer (nullable = true)\n |-- PRIMARYJOBCATEGORY: integer (nullable = true)\n |-- SECONDARYJOBCATEGORY: integer (nullable = true)\n |-- JOBSKILLS: integer (nullable = true)\n |-- STATUS: integer (nullable = true)\n\n"
                }
            ], 
            "source": "dash_users.printSchema()"
        }, 
        {
            "execution_count": 14, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "root\n |-- ITEM_ID: string (nullable = false)\n |-- USER_ID: string (nullable = false)\n |-- ITEM_COMPLETIONDATE: timestamp (nullable = true)\n |-- INTEREST_ID: string (nullable = true)\n |-- INTEREST_LABEL: string (nullable = true)\n\n"
                }
            ], 
            "source": "dash_valid_items.printSchema()"
        }, 
        {
            "source": "### Feature Transform", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 15, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.ml.feature import VectorAssembler, VectorIndexer\nfeaturesCols = dash_users.columns\nfeaturesCols.remove('USER_ID')\n# This concatenates all feature columns into a single feature vector in a new column \"rawFeatures\".\nvectorAssembler = VectorAssembler(inputCols=featuresCols, outputCol=\"rawFeatures\")\n# This identifies categorical features and indexes them.\nvectorIndexer = VectorIndexer(inputCol=\"rawFeatures\", outputCol=\"features\", maxCategories=4284)"
        }, 
        {
            "source": "### Item Transform", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 16, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.ml.feature import StringIndexer\nlabelCol = dash_valid_items.columns\nlabelCol = \"ITEM_ID\"\nlabelIndexer = StringIndexer(inputCol=labelCol, outputCol=\"LABEL_INDEXED\")"
        }, 
        {
            "source": "### Model Building", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 17, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#from pyspark.ml.classification import MultilayerPerceptronClassifier\n#nnc = MultilayerPerceptronClassifier(labelCol=\"LABEL_INDEXED\", layers=[13, 100,  3805], solver=\"gd\", maxIter=2, seed=123)"
        }, 
        {
            "execution_count": 18, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.ml.classification import DecisionTreeClassifier\ndtc = DecisionTreeClassifier(labelCol=\"LABEL_INDEXED\", maxBins=4284)"
        }, 
        {
            "source": "### Pipeline Buidling", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 19, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#from pyspark.ml import Pipeline\n#pipeline_nnc = Pipeline(stages=[vectorAssembler, vectorIndexer, labelIndexer, nnc])"
        }, 
        {
            "execution_count": 20, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.ml import Pipeline\npipeline_dtc = Pipeline(stages=[vectorAssembler, vectorIndexer, labelIndexer, dtc])"
        }, 
        {
            "source": "### Split Data into Training by Topic and Test", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### Project Management", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 21, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "training_data = dash_valid_items.filter(dash_valid_items.ITEM_COMPLETIONDATE < '2018-01-01')\ntraining_data = dash_valid_items.filter(dash_valid_items.INTEREST_LABEL == 'Project Management')"
        }, 
        {
            "execution_count": 22, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "joined_training_data = training_data.join(dash_users, \"USER_ID\", \"right_outer\")"
        }, 
        {
            "execution_count": 23, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "joined_training_data = joined_training_data.fillna({\"ITEM_ID\":\"NAN-0000\"})\n###Find # of distinct LA's\n#joined_training_data.select('ITEM_ID').distinct().rdd.map(lambda r: r[0]).collect()"
        }, 
        {
            "execution_count": 24, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#joined_training_data.count()"
        }, 
        {
            "execution_count": 25, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [
                {
                    "execution_count": 25, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "DataFrame[ITEM_ID: string, USER_ID: string, ITEM_COMPLETIONDATE: timestamp, INTEREST_ID: string, INTEREST_LABEL: string]"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "dash_valid_items.persist()\ntraining_data.persist()"
        }, 
        {
            "execution_count": 26, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": false
            }, 
            "outputs": [], 
            "source": "#pipeline_nnc_model = pipeline_nnc.fit(joined_training_data)\n#pipeline_nnc_model.save(\"pipeline_nnc_model_mobile\")\n#pipeline_nnc_model.write().overwrite().save(\"pipeline_nnc_model_mobile\")"
        }, 
        {
            "execution_count": 27, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "pipeline_dtc_model = pipeline_dtc.fit(joined_training_data)\n#pipeline_dtc_model.save(\"pipeline_dtc_model_mobile\")\npipeline_dtc_model.write().overwrite().save(\"pipeline_dtc_model_mobile\")"
        }, 
        {
            "execution_count": 28, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "prediction = pipeline_dtc_model.transform(dash_users)"
        }, 
        {
            "execution_count": 29, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "source": "#prediction.take(1) "
        }, 
        {
            "execution_count": 30, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "root\n |-- USER_ID: string (nullable = false)\n |-- BAND: integer (nullable = true)\n |-- TENURE: integer (nullable = true)\n |-- BUSINESSUNITGROUP: integer (nullable = true)\n |-- BUSINESSUNIT: integer (nullable = true)\n |-- CITY: integer (nullable = true)\n |-- STATE: integer (nullable = true)\n |-- COUNTRYCODE: integer (nullable = true)\n |-- COUNTRY: integer (nullable = true)\n |-- REGION: integer (nullable = true)\n |-- GEOGRAPHY: integer (nullable = true)\n |-- PRIMARYJOBCATEGORY: integer (nullable = true)\n |-- SECONDARYJOBCATEGORY: integer (nullable = true)\n |-- JOBSKILLS: integer (nullable = true)\n |-- STATUS: integer (nullable = true)\n |-- rawFeatures: vector (nullable = true)\n |-- features: vector (nullable = true)\n |-- rawPrediction: vector (nullable = true)\n |-- probability: vector (nullable = true)\n |-- prediction: double (nullable = true)\n\n"
                }
            ], 
            "source": "prediction.printSchema()"
        }, 
        {
            "execution_count": 31, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#prediction.count()"
        }, 
        {
            "execution_count": 32, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "item_lables = pipeline_dtc_model.stages[2].labels"
        }, 
        {
            "execution_count": 33, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "item_lables_broad = sc.broadcast(item_lables)"
        }, 
        {
            "execution_count": 34, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# from pyspark.sql.functions import udf\n# from pyspark.sql.types import ArrayType,StructType, StructField, StringType, DoubleType\nfrom pyspark.sql import Row\nScore = Row(\"USER_ID\",\"ITEM_ID\", \"ASSOCIATION_SCORE\")\ndef topK(row):\n    arr_v = row[1]\n    arg_indexes = arr_v.argsort()[::-1][:200]\n    item_lables_internal = item_lables_broad.value\n    return [Score(row[0],item_lables_internal[arg_indexes[i]], float(arr_v[arg_indexes[i]])) for i in range(len(arg_indexes))]\n# topK_udf = udf(topK, ArrayType(StructType([StructField(\"ITEM_ID\", StringType()),StructField(\"ITEM_ID\", StringType()),StructField(\"ASSOCIATION_SCORE\", DoubleType())])))"
        }, 
        {
            "execution_count": 35, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def topK_2(row):\n    arr_v = row[\"rawPrediction\"]\n    return (row[\"USER_ID\"],topK(row))"
        }, 
        {
            "execution_count": 36, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "source": "topK_pred = prediction.select(\"USER_ID\", \"probability\").rdd\\\n.flatMap(lambda row: topK(row)).toDF()"
        }, 
        {
            "execution_count": 37, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "topK_pred = topK_pred.filter(topK_pred.ITEM_ID != 'NAN-0000')"
        }, 
        {
            "execution_count": 38, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#topK_pred.take(5)"
        }, 
        {
            "execution_count": 39, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.sql.functions import lit\ntopK_pred = topK_pred.withColumn(\"MODEL\", lit(\"2\"))"
        }, 
        {
            "execution_count": 40, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.sql.functions import desc, rank\nfrom pyspark.sql import Window\nwindow = Window.partitionBy(\"USER_ID\").orderBy(desc(\"ASSOCIATION_SCORE\"))\nt10_scores_df = topK_pred.withColumn(\"RANK\", rank().over(window)).where(\"RANK <= 10\")\nt10_scores_df.write.jdbc(properties_6e54ed0ab7e1422aa339fd754b921514['jdbcurl'], mode = \"append\", table='LXRECOM.WRK_RE5_TOPIC_SPLIT_MODEL_OUTPUT', properties=properties_6e54ed0ab7e1422aa339fd754b921514)\n#t10_scores_df.take(10)"
        }, 
        {
            "execution_count": 41, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#dash_output = spark.read.jdbc(properties_5eb5ff30cc6a4067865f3070299e76a6['jdbcurl'], table='CAO2.LK_TOPIC_SPLIT_MODEL_OUTPUT_SPARK', properties=properties_5eb5ff30cc6a4067865f3070299e76a6)\n#dash_output.head()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark 2.1", 
            "name": "python3-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}